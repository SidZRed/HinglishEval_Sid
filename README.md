# HinglishEval

Artifact for **HinglishEval: Evaluating the Effectiveness of Code-generation Models on Hinglish Prompts**.

## Prompting in Hinglish

To automate the task to translation of the dataset to Hinglish, we have used a primary trusted model - GPT-4 to translate the prompts in OpenAI's [HumanEval Benchmark](https://github.com/openai/human-eval) to _Hinglish_ . This was followed by manual verification of the translations. The benchmark - **HinglishEval** is available as a JSON file in [`HinglishEval.json`](https://github.com/mrigankpawagi/HinglishEval/blob/main/HinglishEval.json).

### Why Hinglish?

Hindi is one of the most widely spoken languages in the world, and the most widely spoen in India. A majority of the population in India does not speak English as their first language, and therefore language models that can understand prompts in native languages are important for wider accessibility. Hinglish is a blend of Hindi and English, with frequent usage of English words in sentences with standard Hindi grammar. This is not representative of everyday spoken Hindi for most people, but is rather common in coversations involving technical language, especially in the context of programming.

Therefore it is most natural for Hindi speaking users to prompt LLMs in Hinglish when they want to generate code, or ask for help with programming in general (like explanations or debugging). This benchmark is an attempt to understand how well LLMs can understand and generate code when prompted in such a language.

## Contributions and Usage

### The HinglishEval Benchmark

The HinglishEval benchmark contains all the problems in the HumnaEval benchmark with their prompts translated to Hinglish. This translation does not modify the function signature or the formats of the doctests and is limited to the purpose statement. THis statement refers to the **docstring** in specific to Python functions. The translations were manually verified and corrected to ensure that they sound like idiomatic Hinglish.

### Code Samples: Model Completions for HinglishEval

We have released code completions generated by 18 different models on the HinglishEval prompts. These completions are available in two forms:

- **Unsanitized Completions**: Found in the `samples/unsanitized` directory, these are raw outputs generated by the models without any post-processing.

- **Sanitized Completions**: Found in the `samples/sanitized directory`.

These outputs are processed to:

- Retain only the requested function definitions.
- Remove any extraneous or irrelevant text.
- Sanitization ensures that only the code relevant to the problem is considered during evaluation.

### Evaluation of Models on HinglishEval

We evaluated 18 models on the HinglishEval dataset using two methodologies:

- Pass@1 Metric:
  This metric evaluates the correctness of model-generated code on the first attempt. All evaluations were conducted using a temperature setting of 0 (greedy decoding).

- Item Response Theory (IRT):
  IRT is a statistical framework used to evaluate both the difficulty of problems and the capabilities of models. It provides deeper insights into:

**Problem Parameters**:

- Difficulty (β): Measures how challenging a problem is.
- Discrimination (α): Measures how well a problem differentiates between high- and low-performing models.
- User Parameters:
  Represents the latent abilities (or competencies) of the models.

## Usages

Here is a brief overview on duplicating the results, running the codes and evaluating the models on the HinglishEval benchmark.

### 1. Cloning the Repository

Clone the repository using the following command:

```bash
git clone https://github.com/mrigankpawagi/HinglishEval.git
cd HinglishEval
```

### 2. Setting up the Environment

Some of the models will be downloaded locally via [HuggingFace](https://huggingface.co/models) like codegen, polycoder, gemma, etc. while others like gpt, mistral, llama, etc. use API-KEYS to access these models. We use mainly two distributors which are [OpenAI](https://openai.com) and [DeepInfra](https://deepinfra.com) You have to setup your own API Keys and set them in the `.env` file as present in the cloned repository.
<br>

You can find the link to obtain the API Keys for [OPENAI](https://openai.com/index/openai-api/), [DEEPINFRA](https://deepinfra.com/docs/deep_infra_api) and [HUGGING FACE](https://huggingface.co/docs/api-inference/en/index) in the respective links.

<br>
Next thing to do is to install some requirements which are necessary to run the code. You can create a virtual environment and install the requirements using the following commands:

```bash
python3 -m venv hinglisheval
source hinglisheval/bin/activate
pip install -r requirements.txt
```

Lastly(optional), we need to install `[HumanEval.json](https://github.com/openai/human-eval/blob/master/data)` file if you are interested in generating `English` generations or reprodice `HinglishEval.json`. This can be done by running the following command:

```bash
curl -O https://github.com/openai/human-eval/raw/refs/heads/master/data/HumanEval.jsonl.gz
gunzip HumanEval.jsonl.gz
```

### 3. Running the Code

- You can run and obtain code samples of various models by running codes in the `[samples/codemodels](/samples/codemodels)` directory.
- You can generate the HinglishEval dataset by running the code in the `[samples/tools](/samples/tools)` directory.
